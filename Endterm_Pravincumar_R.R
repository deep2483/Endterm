#Question 1

##Importing the dataset and using str()
setwd("C:/Users/pravincumar/Desktop/Praxis BA/DM2")

lens<- read.csv("test.csv",sep=",",header=F,stringsAsFactors =F)
head(lens)
class(lens)
str(lens)
dim(lens)

##Adding the Column names
colnames(lens) = c('index','age','spec_pres','astigmatic','tpr','Class')
lenses<-lens
head(lenses)
str(lens)

##Changing the data to its names for the age and spec_pres variables
lenses$age[lenses$age=="1"]="young"
lenses$age[lenses$age=="2"]="pre-presbyopic"
lenses$age[lenses$age=="3"]="presbyopic"
lenses$spec_pres[lenses$spec_pres=="1"]="myope"
lenses$spec_pres[lenses$spec_pres=="2"]="hypermetrope"
str(lenses)

##Changing the class for astigmatic variable
len<-lenses
len$astigmatic<-as.character(len$astigmatic)
str(len$astigmatic)
##Finding the frequency to get the mode value for replacing the missing values 
table(len$astigmatic)
##Replacing by mode value

len$astigmatic[len$astigmatic==""]="1"


##Changing the  astigmatic variable to right names
len$astigmatic[len$astigmatic=="1"]="NO"
len$astigmatic[len$astigmatic=="2"]="YES"
View(len)

##Changing to names in tpr column
len$tpr[len$tpr==1]="reduced"
str(len)
len$tpr[len$tpr==2]="normal"
table(len$tpr)

##replacing the g value with mode in astigmatic variable
##Finding the frequency to get the mode value for replacing the missing values 
table(len$astigmatic)
len$astigmatic[len$astigmatic=="g"]="NO"
View(len)

##removing index column

lenses<-len[-1]
head(lenses)  

## Correct 10/10
#Question 2
library(tree)

library(rattle)
rattle()

lenses$Class<-as.factor(lenses$Class)
lenses$age<-as.factor(lenses$age)
lenses$spec_pres<-as.factor(lenses$spec_pres)
lenses$astigmatic<-as.factor(lenses$astigmatic)
lenses$tpr<-as.factor(lenses$tpr)


is.factor(lenses$Class)
is.factor(lenses$age)
is.factor(lenses$spec_pres)
is.factor(lenses$astigmatic)
is.factor(lenses$tpr)

##Split data into testing and training 
set.seed(2)
train=sample(1:nrow(lenses),nrow(lenses)*0.5)
test=~train
training_data= lenses[train,]
testing_data= lenses[-train,]
dim(training_data)
dim(testing_data)

#Using tree() to build a decision tree on training and store the model as model1
model1 = tree(Class~ .,data = training_data)

##Ploting the tee
plot(model1)
text(model1,pretty =0)

##predicting the test dataset
pred_class = predict(model1,data=testing_data,type="class")
plot(pred_class,testing_data$Class)
abline (0,1)
##Confusion Matrix
table(pred_class,testing_data$Class)
summary(pred_class)
mean(pred_class != testing_data$Class)
##Accuracy is 41.6% and misclassification error rate in 58.3% both looks poor and worse
##We can improve the model by using C5.0 which has inbuilt boosting or by using Random forest
## Correct 10/0


#Question 3

foods<- read.csv("food.csv",sep=",",header=T,stringsAsFactors =F)
head(foods)

row.names(foods)<-foods[,1]
head(foods)

##Treating missing Values
b<- table(is.na(foods)) 
missingtreat<-function(df)
{
  for(i in 1:length(df))
  { 
    if(is.numeric(df[,i]))
    {
      dat1<-df[,i]
      dat1[is.na(dat1)]<-mean(df[,i],na.rm=T)
      df[,i]<-dat1
      
    }
    else{o=table(df[,i])}
  }
  o<-df
  return(o)
}

foo<-missingtreat(foods)
c<- table(is.na(foo)) 
##Fiting PCA

pr.out =prcomp (foo , scale =TRUE)

names(pr.out)
pr.out$center
pr.out$scale

pr.out$rotation

##Ploting the loadings of PC1 against PC2
pr.out$x
biplot (pr.out , scale =0)
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot (pr.out , scale =0)

##Calculating the Variance generated by principal components
pr.out$sdev
pr.var =pr.out$sdev ^2
pr.var

##Intepretation: The first two PCS has Eigen values greater than 3 and pc3 has eigen value greater than 2
##therefore we can consider using the first three pcs.
##PC1 NAME- FRESH PRODUCTS, PC2 NAME - OILY Products

## Correct 10/10


#question 4 

library(MASS)
library(ROCR)
library(pROC)
library(ISLR)

attach(iris)
head(iris)
data1<-iris

dim(iris)
library(rpart)
library(caret)
set.seed(1)
intrain <- createDataPartition(y=data1$Species, p=0.80, list=F)
train <- data1[intrain,]
test <- data1[-intrain,]

dim(train)


lda.fit=lda(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=train)
lda.fit

plot(lda.fit)

lda.pred1=predict(lda.fit , test)
lda.class =lda.pred1$class
#Posterior Probabilities

head(lda.pred1$posterior)

#Plot lda1 & lda2 and coloured based on Species
plot(lda.pred1$x,col=Species)



#Confussion Martix
table(lda.class,test$Species)

mean(lda.class != test$Species)
#Misclassification error rate <- 0.03

## Correct 10/10
